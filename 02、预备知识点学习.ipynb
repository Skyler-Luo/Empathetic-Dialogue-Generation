{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预备知识点学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transformer 模型是本项目的核心，接下来，我们将从机制原理、核心组件、对话场景适配等方面逐步学习如何搭建出本项目的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Transformer 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "![Transformer 工作流程图](./images/02-1.png)\n",
    "\n",
    "**Transformer** 模型在2017年由 google 提出，直接基于 Self-Attention 结构，取代了之前 NLP 任务中常用的 RNN 神经网络结构，并在 WMT2014 Englishto-German 和 WMT2014 English-to-French 两个机器翻译任务上都取得了当时的 **SOTA**(State of the Art)。\n",
    "\n",
    "论文链接：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer通过引入注意力机制来跟踪序列数据中的关系，从而学习上下文并理解含义。与传统的递归神经网络（RNN）或卷积神经网络（CNN）不同，它能够并行处理序列数据，显著提高了训练效率。这种架构的关键在于自注意力机制，它允许模型同时分析序列中的所有元素，而不仅仅是顺序处理，从而捕捉长距离的依赖关系。\n",
    "\n",
    "Transformer模型由输入嵌入层、编码器、解码器以及输出层组成，其中编码器和解码器都包含多头注意力机制和前馈神经网络。位置编码被添加到输入序列中，以提供关于词序的信息，因为Transformer本身不具有顺序处理的能力。\n",
    "\n",
    "这种架构的出现极大地推动了自然语言处理（NLP）领域的发展，是BERT、GPT等现代大型语言模型的基础。此外，Transformer主要应用于处理序列数据，比如自然语言文本，现已经扩展到计算机视觉领域，如ViT（Vision Transformer），显示了其在不同领域的广泛适用性。\n",
    "\n",
    "Transformer 包括以下关键组件：\n",
    "- **Self-Attention**：在序列内部建模相关性\n",
    "- **Multi-Head**：并行多组注意力，学习互补关系\n",
    "- **Positional Encoding**：为无序的注意力引入位置信息\n",
    "- **Feed-Forward + Residual + LayerNorm**：稳定训练与非线性建模\n",
    "- **Masking**：屏蔽 PAD 与未来信息（解码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### 2.1 自注意力机制(Self-Attention)\n",
    "\n",
    "自注意力机制(Self-Attention) 是Transformer模型的核心组成部分，它允许模型在处理序列数据时能够关注到输入序列中最重要的部分，而不依赖于固定的顺序或距离。\n",
    "\n",
    "注意力机制有三个核心变量：**$Query$**（查询值）、**$Key$**（键值）和 **$Value$**（真值）。自注意力机制的核心思想是在处理序列数据时，序列中的每个词都同时作为查询（$Query$）、键（$Key$）和值（$Value$），通过计算每个词与所有其他词之间的相关性得到一个注意力权重，以表示其重要性。具体来说注意力机制可以分为以下几个步骤：\n",
    "\n",
    "![注意力机制](./images/02-2.png)\n",
    "\n",
    "1. **计算查询（Query）、键（Key）、值（Value）向量：** \n",
    "\n",
    "    假设输入序列为 $X = [x_1, x_2, \\ldots, x_n]$，通过权重矩阵线性变换得到：  \n",
    "    $$Q_i = W_q \\cdot x_i, \\quad K_i = W_K \\cdot x_i, \\quad V_i = W_V \\cdot x_i$$\n",
    "    其中 $Q_i, K_i, V_i$ 分别表示第 $i$ 个元素的查询、键和值向量。\n",
    "2. **计算注意力分数：**\n",
    "    \n",
    "    对每一对元素 $(x_i, x_j)$，计算点积并进行缩放（$d_k$ 为键向量维度），得到注意力分数，该值表示二者间的相关程度：\n",
    "    $$\\text{score}(Q_i, K_j) = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}$$\n",
    "3. **计算注意力权重：**\n",
    "    通过 $softmax$ 函数对注意力分数进行归一化，得到权重 $w_{ij}$（使用 $softmax$ 函数是为了使得所有的注意力权重在 $[0,1]$ 之间，且和为 1。其可以帮助模型在处理数据时，聚焦于重要的部分，忽略无关的信息。）：\n",
    "    $$w_{ij} = \\text{softmax}\\left(\\text{score}(Q_i, K_j)\\right)$$\n",
    "4. **加权求和：**  \n",
    "    将数据序列中的每个元素与其注意力权重进行加权求和，生成上下文相关的输出 $z_i$,输出向量融合了全局上下文信息。\n",
    "    $$z_i = \\sum_{j=1}^n w_{ij} V_j$$\n",
    "\n",
    "综上所述，缩放点积自注意力机制可表示为（$Q$、$K$、$V$ 均为矩阵）：$$\\text{Output} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs:\n",
    "    def __init__(self, vocab_size=None, block_size=128, n_layer=2, n_heads=2, n_embd=128, dropout=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.max_seq_len = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embd = n_embd\n",
    "        self.dim = n_embd\n",
    "        self.dropout = dropout\n",
    "\n",
    "args = ModelArgs(\n",
    "    vocab_size=1000,\n",
    "    block_size=16,\n",
    "    n_layer=2,\n",
    "    n_heads=2,\n",
    "    n_embd=64,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SelfAttention] input: torch.Size([2, 5, 16]) output: torch.Size([2, 5, 16]) attn: torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        Q = self.query(x)  # (B, L, embed_size)\n",
    "        K = self.key(x)     # (B, L, embed_size)\n",
    "        V = self.value(x)   # (B, L, embed_size)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.embed_size))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# 示例：SelfAttention 输入/输出\n",
    "x = torch.randn(2, 5, 16)\n",
    "sa = SelfAttention(embed_size=16)\n",
    "sa_out, sa_attn = sa(x)\n",
    "print('[SelfAttention] input:', x.shape, 'output:', sa_out.shape, 'attn:', sa_attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 掩码自注意力\n",
    "\n",
    "掩码自注意力，即 Mask Self-Attention，是指使用注意力掩码的自注意力机制。掩码的作用是遮蔽一些特定位置的 token，模型在学习的过程中，会忽略掉被遮蔽的 token。\n",
    "\n",
    "使用注意力掩码的核心动机是让模型只能使用历史信息进行预测而不能看到未来信息。Transformer 模型对一个文本序列，会不断根据之前的 token 来预测下一个 token，直到将整个文本序列补全。\n",
    "\n",
    "例如，如果待学习的文本序列是 【BOS】I like you【EOS】（ BOS表示Begin of Sentence，EOS表示End of Sentence），那么，模型会按如下顺序进行预测和学习：\n",
    "\n",
    "    Step 1：输入 【BOS】，输出 I\n",
    "    Step 2：输入 【BOS】I，输出 like\n",
    "    Step 3：输入 【BOS】I like，输出 you\n",
    "    Step 4：输入 【BOS】I like you，输出 【EOS】\n",
    "\n",
    "\n",
    "我们可以发现，上述过程是一个串行的过程，也就是需要先完成 Step 1，才能做 Step 2，接下来逐步完成整个序列的补全。如果对于每一个训练语料，模型都需要串行完成上述过程才能完成学习，那么很明显没有做到并行计算，计算效率很低。那么，如何解决呢？\n",
    "\n",
    "针对这个问题，Transformer 就提出了掩码自注意力的方法。掩码自注意力会生成一串掩码，来遮蔽未来信息。例如，待学习的文本序列仍然是 【BOS】I like you【EOS】，使用的注意力掩码是【MASK】，那么模型的输入为：\n",
    "\n",
    "    <BOS> 【MASK】【MASK】【MASK】【MASK】\n",
    "    <BOS>    I   【MASK】 【MASK】【MASK】\n",
    "    <BOS>    I     like  【MASK】【MASK】\n",
    "    <BOS>    I     like    you  【MASK】\n",
    "    <BOS>    I     like    you   <EOS>\n",
    "\n",
    "在每一行输入中，模型仍然是只看到前面的 token，预测下一个 token。但是，上述输入不再是串行的过程，而可以一起并行地输入到模型中，模型只需要每一个样本根据未被遮蔽的 token 来预测下一个 token 即可，从而实现了并行的语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察上述的掩码，我们可以发现其实则是一个和文本序列等长的上三角矩阵。我们可以简单地通过创建一个和输入同等长度的上三角矩阵作为注意力掩码，再使用掩码来遮蔽掉输入即可。也就是说，当输入维度为 （batch_size, seq_len, hidden_size）时，我们的 Mask 矩阵维度一般为 (1, seq_len, seq_len)（通过广播实现同一个 batch 中不同样本的计算）。\n",
    "\n",
    "在具体实现中，我们通过以下代码生成 Mask 矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.full((1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))  # full 函数创建一个 1 * seq_len * seq_len 的矩阵\n",
    "mask = torch.triu(mask, diagonal=1)  # triu 函数的功能是创建一个上三角矩阵\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的 Mask 矩阵会是一个上三角矩阵，上三角位置的元素均为 -inf，其他位置的元素置为0。\n",
    "\n",
    "在注意力计算时，我们会将计算得到的注意力分数与这个掩码做和，再进行 Softmax 操作：\n",
    "\n",
    "```python\n",
    "# 此处的 scores 为计算得到的注意力分数，mask 为上文生成的掩码矩阵\n",
    "scores = scores + mask[:, :seqlen, :seqlen]\n",
    "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过做求和，上三角区域（也就是应该被遮蔽的 token 对应的位置）的注意力分数结果都变成了 `-inf`，而下三角区域的分数不变。再做 Softmax 操作，`-inf` 的值在经过 Softmax 之后会被置为 0，从而忽略了上三角区域计算的注意力分数，从而实现了注意力遮蔽。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 多头注意力\n",
    "\n",
    "注意力机制可以实现并行化与长期依赖关系拟合，但一次注意力计算只能拟合一种相关关系，单一的注意力机制很难全面拟合语句序列里的相关关系。因此 Transformer 使用了多头注意力机制（Multi-Head Attention），即同时对一个语料进行多次注意力计算，每次注意力计算都能拟合不同的关系，将最后的多次结果拼接起来作为最后的输出，即可更全面深入地拟合语言信息。\n",
    "\n",
    "多头注意力机制其实就是将原始的输入序列进行多组的自注意力处理；然后再将每一组得到的自注意力结果拼接起来，再通过一个线性层进行处理，得到最终的输出。我们用公式可以表示为：\n",
    "\n",
    "$$\n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n",
    "\n",
    "我们可以通过矩阵运算巧妙地实现并行的多头计算，其核心逻辑在于使用三个组合矩阵来代替了n个参数矩阵的组合，也就是矩阵内积再拼接其实等同于拼接矩阵再内积。具体实现可以参考下列代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''多头自注意力计算模块'''\n",
    "    def __init__(self, args, is_causal=False):\n",
    "        # 构造函数\n",
    "        # args: 配置对象\n",
    "        super().__init__()\n",
    "        self.is_causal = is_causal\n",
    "        # 隐藏层维度必须是头数的整数倍，因为后面我们会将输入拆成头数个矩阵\n",
    "        assert args.dim % args.n_heads == 0\n",
    "        # 模型并行处理大小，默认为1。\n",
    "        model_parallel_size = 1\n",
    "        # 本地计算头数，等于总头数除以模型并行处理大小。\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # 每个头的维度，等于模型维度除以头的总数。\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # Wq, Wk, Wv 参数矩阵，每个参数矩阵为 n_embd x n_embd\n",
    "        # 这里通过三个组合矩阵来代替了n个参数矩阵的组合，其逻辑在于矩阵内积再拼接其实等同于拼接矩阵再内积，\n",
    "        self.wq = nn.Linear(args.dim, self.n_local_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_local_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_local_heads * self.head_dim, bias=False)\n",
    "        # 输出权重矩阵，维度为 dim x n_embd（head_dim = n_embeds / n_heads）\n",
    "        self.wo = nn.Linear(self.n_local_heads * self.head_dim, args.dim, bias=False)\n",
    "        # 注意力的 dropout\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        # 残差连接的 dropout\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        \n",
    "        # 创建一个上三角矩阵，用于遮蔽未来信息\n",
    "        # 注意，因为是多头注意力，Mask 矩阵比之前我们定义的多一个维度\n",
    "        if is_causal:\n",
    "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            # 注册为模型的缓冲区\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):\n",
    "\n",
    "        # 获取批次大小和序列长度，[batch_size, seq_len, dim]\n",
    "        bsz, seqlen, _ = q.shape\n",
    "\n",
    "        # 计算查询（Q）、键（K）、值（V）,输入通过参数矩阵层，维度为 (B, T, n_embed) x (n_embed, n_embed) -> (B, T, n_embed)\n",
    "        xq, xk, xv = self.wq(q), self.wk(k), self.wv(v)\n",
    "\n",
    "        # 将 Q、K、V 拆分成多头，维度为 (B, T, n_head, C // n_head)，然后交换维度，变成 (B, n_head, T, C // n_head)\n",
    "        # 因为在注意力计算中我们是取了后两个维度参与计算\n",
    "        # 为什么要先按B*T*n_head*C//n_head展开再互换1、2维度而不是直接按注意力输入展开，是因为view的展开方式是直接把输入全部排开，\n",
    "        # 然后按要求构造，可以发现只有上述操作能够实现我们将每个头对应部分取出来的目标\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # 注意力计算\n",
    "        # 计算 QK^T / sqrt(d_k)，维度为 (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # 掩码自注意力必须有注意力掩码\n",
    "        if self.is_causal:\n",
    "            assert hasattr(self, 'mask')\n",
    "            # 这里截取到序列长度，因为有些序列可能比 max_seq_len 短\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "        # 计算 softmax，维度为 (B, nh, T, T)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        # 做 Dropout\n",
    "        scores = self.attn_dropout(scores)\n",
    "        # V * Score，维度为(B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        output = torch.matmul(scores, xv)\n",
    "\n",
    "        # 恢复时间维度并合并头。\n",
    "        # 将多头的结果拼接起来, 先交换维度为 (B, T, n_head, C // n_head)，再拼接成 (B, T, n_head * C // n_head)\n",
    "        # contiguous 函数用于重新开辟一块新内存存储，因为Pytorch设置先transpose再view会报错，\n",
    "        # 因为view直接基于底层存储得到，然而transpose并不会改变底层存储，因此需要额外存储\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        # 最终投影回残差流。\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MHA non-causal] input: torch.Size([2, 5, 64]) output: torch.Size([2, 5, 64])\n",
      "[MHA causal] input: torch.Size([2, 5, 64]) output: torch.Size([2, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "# 示例：MultiHeadAttention 输入/输出（非因果与因果）\n",
    "# 使用上文定义的 ModelArgs（args）\n",
    "\n",
    "# 构造一个简单的输入：batch_size=2, seq_len=5, dim=args.dim\n",
    "x = torch.randn(2, 5, args.dim)\n",
    "\n",
    "# 非因果（Self-Attention 类似 Encoder 内部）\n",
    "mha_non_causal = MultiHeadAttention(args, is_causal=False)\n",
    "y_non = mha_non_causal(x, x, x)\n",
    "print('[MHA non-causal] input:', x.shape, 'output:', y_non.shape)\n",
    "\n",
    "# 因果（Mask Self-Attention，类似 Decoder 第一个子层）\n",
    "mha_causal = MultiHeadAttention(args, is_causal=True)\n",
    "y_causal = mha_causal(x, x, x)\n",
    "print('[MHA causal] input:', x.shape, 'output:', y_causal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 3. 位置编码\n",
    "\n",
    "自注意力对顺序不敏感，需要引入位置信息。经典的正弦/余弦位置编码：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{PE}(pos,2i) &= \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\\\\n",
    "\\mathrm{PE}(pos,2i+1) &= \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "​上式中，$pos$ 为 $token$ 在句子中的位置，$d_{model}$代表位置向量的维度，$i \\in [0, d_{model})$代表位置$d_{model}$维位置向量第$i$维。$2i$ 和 $2i+1$ 则是指示了 token 是奇数位置还是偶数位置，从上式中我们可以看出对于奇数位置的 $token$ 和偶数位置的 $token$，Transformer 采用了不同的函数进行编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们以一个简单的例子来说明位置编码的计算过程：假如我们输入的是一个长度为 4 的句子\"I like to code\"，我们可以得到下面的词向量矩阵 $\\rm x$ ，其中每一行代表的就是一个词向量， $\\rm x_0=[0.1,0.2,0.3,0.4]$ 对应的就是“I”的词向量，它的 $pos$ 就是为0，以此类推，第二行代表的是“like”的词向量，它的 $pos$ 就是1：\n",
    "\n",
    "$$\n",
    "\\rm x = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.3 & 0.4 & 0.5 & 0.6 \\\\ 0.4 & 0.5 & 0.6 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "​则经过位置编码后的词向量为：\n",
    "\n",
    "$$\n",
    "\\rm x_{PE} = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.3 & 0.4 & 0.5 & 0.6 \\\\ 0.4 & 0.5 & 0.6 & 0.7 \\end{bmatrix} + \\begin{bmatrix} \\sin(\\frac{0}{10000^0}) & \\cos(\\frac{0}{10000^0}) & \\sin(\\frac{0}{10000^{2/4}}) & \\cos(\\frac{0}{10000^{2/4}}) \\\\ \\sin(\\frac{1}{10000^0}) & \\cos(\\frac{1}{10000^0}) & \\sin(\\frac{1}{10000^{2/4}}) & \\cos(\\frac{1}{10000^{2/4}}) \\\\ \\sin(\\frac{2}{10000^0}) & \\cos(\\frac{2}{10000^0}) & \\sin(\\frac{2}{10000^{2/4}}) & \\cos(\\frac{2}{10000^{2/4}}) \\\\ \\sin(\\frac{3}{10000^0}) & \\cos(\\frac{3}{10000^0}) & \\sin(\\frac{3}{10000^{2/4}}) & \\cos(\\frac{3}{10000^{2/4}}) \\end{bmatrix} = \\begin{bmatrix} 0.1 & 1.2 & 0.3 & 1.4 \\\\ 1.041 & 0.84 & 0.41 & 1.49 \\\\ 1.209 & -0.016 & 0.52 & 1.59 \\\\ 0.541 & -0.489 & 0.895 & 1.655 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "我们可以使用如下的代码来获取上述例子的位置编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.09983342  0.99500417]\n",
      " [ 0.90929743 -0.41614684  0.19866933  0.98006658]\n",
      " [ 0.14112001 -0.9899925   0.29552021  0.95533649]]\n"
     ]
    }
   ],
   "source": [
    "def PositionEncoding(seq_len, d_model, n=10000):\n",
    "    P = np.zeros((seq_len, d_model))\n",
    "    for k in range(seq_len):\n",
    "        for i in np.arange(int(d_model/2)):\n",
    "            denominator = np.power(n, 2*i/d_model)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return P\n",
    "\n",
    "P = PositionEncoding(seq_len=4, d_model=4, n=100)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于上述原理，我们实现一个​位置编码层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PositionalEncoding] input: torch.Size([2, 5, 64]) output: torch.Size([2, 5, 64])\n",
      "mean |y - x|: 0.5410805940628052\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''位置编码模块'''\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Dropout 层\n",
    "        # self.dropout = nn.Dropout(p=args.dropout)\n",
    "\n",
    "        # block size 是序列的最大长度\n",
    "        pe = torch.zeros(args.block_size, args.n_embd)\n",
    "        position = torch.arange(0, args.block_size).unsqueeze(1)\n",
    "        # 计算 theta\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, args.n_embd, 2) * -(math.log(10000.0) / args.n_embd)\n",
    "        )\n",
    "        # 分别计算 sin、cos 结果\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将位置编码加到 Embedding 结果上\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 示例：PositionalEncoding 输出\n",
    "pe_layer = PositionalEncoding(args)\n",
    "# 构造一个简单的输入：batch_size=2, seq_len=5, hidden=args.n_embd\n",
    "x = torch.randn(2, 5, args.n_embd)\n",
    "y = pe_layer(x.clone())\n",
    "print('[PositionalEncoding] input:', x.shape, 'output:', y.shape)\n",
    "print('mean |y - x|:', (y - x).abs().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下图中，我们画出了一种位置向量在第4、5、6、7维度、不同位置的的数值大小。横坐标表示位置下标，纵坐标表示数值大小。\n",
    "\n",
    "![位置编码](./images/02-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transformer 架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Transformer模型中，Encoder 和 Decoder 是两个核心组成部分，主要用于处理输入数据并生成输出。如果我们将模型看为一个黑匣子，那么编码器和解码器连接类似与下面一张图片展示了 Transformer 模型的详细结构图，分为编码器（Encoder）和解码器（Decoder）两部分。编码器负责将输入序列转换为上下文感知的表示，而解码器则根据编码器的输出和自身的输入生成目标序列。\n",
    "\n",
    "![Transformer 结构图](./images/02-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 编码器（Encoder）\n",
    "\n",
    "Encoder 由 N 个 Encoder Layer 组成，每一个 Encoder Layer 包括一个注意力层（Multi-Head Self-Attention）和一个前馈神经网络（Feed-Forward Neural Network）。每个子模块的输出通过残差连接（Residual Connection）与输入相加，并通过层归一化（Layer Normalization）进行标准化，以加速训练和提高稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, dropout: float):\n",
    "        super().__init__()\n",
    "        hidden_features = max(4 * in_features, out_features)\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    ''' Layer Norm 层'''\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # 线性矩阵做映射\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 在统计每个样本所有维度的值，求均值和方差\n",
    "        mean = x.mean(-1, keepdim=True) # mean: [bsz, max_len, 1]\n",
    "        std = x.std(-1, keepdim=True) # std: [bsz, max_len, 1]\n",
    "        # 注意这里也在最后一个维度发生了广播\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    '''Encoder层'''\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # 一个 Layer 中有两个 LayerNorm，分别在 Attention 之前和 MLP 之前\n",
    "        self.attention_norm = LayerNorm(args.n_embd)\n",
    "        # Encoder 不需要掩码，传入 is_causal=False\n",
    "        self.attention = MultiHeadAttention(args, is_causal=False)\n",
    "        self.fnn_norm = LayerNorm(args.n_embd)\n",
    "        self.feed_forward = MLP(args.dim, args.dim, args.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer Norm\n",
    "        norm_x = self.attention_norm(x)\n",
    "        # 自注意力\n",
    "        h = x + self.attention.forward(norm_x, norm_x, norm_x)\n",
    "        # 经过前馈神经网络\n",
    "        out = h + self.feed_forward.forward(self.fnn_norm(h))\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''Encoder 块'''\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__() \n",
    "        # 一个 Encoder 由 N 个 Encoder Layer 组成\n",
    "        self.layers = nn.ModuleList([EncoderLayer(args) for _ in range(args.n_layer)])\n",
    "        self.norm = LayerNorm(args.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"分别通过 N 层 Encoder Layer\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 解码器（Decoder）\n",
    "\n",
    "类似的，我们也可以先搭建 Decoder Layer，再将 N 个 Decoder Layer 组装为 Decoder。但是和 Encoder 不同的是，Decoder 由两个注意力层和一个前馈神经网络组成。第一个注意力层是一个掩码自注意力层，即使用 Mask 的注意力计算，保证每一个 token 只能使用该 token 之前的注意力分数；第二个注意力层是一个多头注意力层，该层将使用第一个注意力层的输出作为 query，使用 Encoder 的输出作为 key 和 value，来计算注意力分数。最后，再经过前馈神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''解码层'''\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # 一个 Layer 中有三个 LayerNorm，分别在 Mask Attention 之前、Self Attention 之前和 MLP 之前\n",
    "        self.attention_norm_1 = LayerNorm(args.n_embd)\n",
    "        # Decoder 的第一个部分是 Mask Attention，传入 is_causal=True\n",
    "        self.mask_attention = MultiHeadAttention(args, is_causal=True)\n",
    "        self.attention_norm_2 = LayerNorm(args.n_embd)\n",
    "        # Decoder 的第二个部分是 类似于 Encoder 的 Attention，传入 is_causal=False\n",
    "        self.attention = MultiHeadAttention(args, is_causal=False)\n",
    "        self.ffn_norm = LayerNorm(args.n_embd)\n",
    "        # 第三个部分是 MLP\n",
    "        self.feed_forward = MLP(args.dim, args.dim, args.dropout)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        # Layer Norm\n",
    "        norm_x = self.attention_norm_1(x)\n",
    "        # 掩码自注意力\n",
    "        x = x + self.mask_attention.forward(norm_x, norm_x, norm_x)\n",
    "        # 多头注意力\n",
    "        norm_x = self.attention_norm_2(x)\n",
    "        h = x + self.attention.forward(norm_x, enc_out, enc_out)\n",
    "        # 经过前馈神经网络\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''解码器'''\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__() \n",
    "        # 一个 Decoder 由 N 个 Decoder Layer 组成\n",
    "        self.layers = nn.ModuleList([DecoderLayer(args) for _ in range(args.n_layer)])\n",
    "        self.norm = LayerNorm(args.n_embd)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoder] input: torch.Size([2, 5, 64]) output: torch.Size([2, 5, 64])\n",
      "[Decoder] input: torch.Size([2, 5, 64]) output: torch.Size([2, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "# 构造输入\n",
    "x = torch.randn(2, 5, args.n_embd)\n",
    "\n",
    "# Encoder 输入/输出\n",
    "encoder = Encoder(args)\n",
    "enc_out = encoder(x)\n",
    "print('[Encoder] input:', x.shape, 'output:', enc_out.shape)\n",
    "\n",
    "# Decoder 输入/输出（使用上面 encoder 的输出作为 KV）\n",
    "decoder = Decoder(args)\n",
    "dec_out = decoder(x, enc_out)\n",
    "print('[Decoder] input:', x.shape, 'output:', dec_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 简易 Transformer\n",
    "\n",
    "基于之前所实现过的组件，我们就组合出一个简易的 Transformer 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''整体模型'''\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # 必须输入词表大小和 block size\n",
    "        assert args.vocab_size is not None\n",
    "        assert args.block_size is not None\n",
    "        self.args = args\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(args.vocab_size, args.n_embd),\n",
    "            wpe = PositionalEncoding(args),\n",
    "            drop = nn.Dropout(args.dropout),\n",
    "            encoder = Encoder(args),\n",
    "            decoder = Decoder(args),\n",
    "        ))\n",
    "        # 最后的线性层，输入是 n_embd，输出是词表大小\n",
    "        self.lm_head = nn.Linear(args.n_embd, args.vocab_size, bias=False)\n",
    "\n",
    "        # 初始化所有的权重\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # 查看所有参数的数量\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    '''统计所有参数的数量'''\n",
    "    def get_num_params(self, non_embedding=False):\n",
    "        # non_embedding: 是否统计 embedding 的参数\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        # 如果不统计 embedding 的参数，就减去\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wte.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    '''初始化权重'''\n",
    "    def _init_weights(self, module):\n",
    "        # 线性层和 Embedding 层初始化为正则分布\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    '''前向计算函数'''\n",
    "    def forward(self, idx, targets=None):\n",
    "        # 输入为 idx，维度为 (batch size, sequence length, 1)；targets 为目标序列，用于计算 loss\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.args.block_size, f\"不能计算该序列，该序列长度为 {t}, 最大序列长度只有 {self.args.block_size}\"\n",
    "\n",
    "        # 通过 self.transformer\n",
    "        # 首先将输入 idx 通过 Embedding 层，得到维度为 (batch size, sequence length, n_embd)\n",
    "        print(\"idx\",idx.size())\n",
    "        # 通过 Embedding 层\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        print(\"tok_emb\",tok_emb.size())\n",
    "        # 然后通过位置编码\n",
    "        pos_emb = self.transformer.wpe(tok_emb) \n",
    "        # 再进行 Dropout\n",
    "        x = self.transformer.drop(pos_emb)\n",
    "        # 然后通过 Encoder\n",
    "        print(\"x after wpe:\",x.size())\n",
    "        enc_out = self.transformer.encoder(x)\n",
    "        print(\"enc_out:\",enc_out.size())\n",
    "        # 再通过 Decoder\n",
    "        x = self.transformer.decoder(x, enc_out)\n",
    "        print(\"x after decoder:\",x.size())\n",
    "\n",
    "        if targets is not None:\n",
    "            # 训练阶段，如果我们给了 targets，就计算 loss\n",
    "            # 先通过最后的 Linear 层，得到维度为 (batch size, sequence length, vocab size)\n",
    "            logits = self.lm_head(x)\n",
    "            # 再跟 targets 计算交叉熵\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # 推理阶段，我们只需要 logits，loss 为 None\n",
    "            # 取 -1 是只取序列中的最后一个作为输出\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们使用随机生成的 token 序列进行下一词预测演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.36M\n",
      "idx torch.Size([4, 16])\n",
      "tok_emb torch.Size([4, 16, 64])\n",
      "x after wpe: torch.Size([4, 16, 64])\n",
      "enc_out: torch.Size([4, 16, 64])\n",
      "x after decoder: torch.Size([4, 16, 64])\n",
      "step 01 | loss 6.9663\n",
      "idx torch.Size([4, 16])\n",
      "tok_emb torch.Size([4, 16, 64])\n",
      "x after wpe: torch.Size([4, 16, 64])\n",
      "enc_out: torch.Size([4, 16, 64])\n",
      "x after decoder: torch.Size([4, 16, 64])\n",
      "step 02 | loss 6.9663\n",
      "idx torch.Size([4, 16])\n",
      "tok_emb torch.Size([4, 16, 64])\n",
      "x after wpe: torch.Size([4, 16, 64])\n",
      "enc_out: torch.Size([4, 16, 64])\n",
      "x after decoder: torch.Size([4, 16, 64])\n",
      "step 03 | loss 6.9190\n",
      "done, elapsed: 0.12s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 复用上文 args；如需更小词表可自行调整\n",
    "model = Transformer(args).to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = args.block_size\n",
    "num_steps = 3  # 演示用，步数少以减少输出\n",
    "\n",
    "start = time.time()\n",
    "for step in range(1, num_steps + 1):\n",
    "    # 构造随机序列（[0, vocab_size) 的 token id）\n",
    "    idx = torch.randint(0, args.vocab_size, (batch_size, seq_len), dtype=torch.long, device=device)\n",
    "    # 目标为下一词预测：targets[t] = idx[t+1]，最后一个位置忽略\n",
    "    targets = idx.clone()\n",
    "    targets[:, :-1] = idx[:, 1:]\n",
    "    targets[:, -1] = -1  # ignore_index\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    logits, loss = model(idx, targets)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'step {step:02d} | loss {loss.item():.4f}')\n",
    "\n",
    "print('done, elapsed: %.2fs' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的Transformer模型代码详见 ./Model/transformer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
