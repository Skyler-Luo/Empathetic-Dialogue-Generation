{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础对话模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "本节基于上一章的 Transformer 基础，系统讲解如何把通用的 `Transformer` 模型改进为适应本项目的共情对话模型：在不破坏原有 Encoder/Decoder 主干的前提下，加入情感分类头与相应的训练目标，并对接情感词典，形成“情感引导”的对话生成。\n",
    "\n",
    "注：以下所有内容，仅包含核心代码供讲解所需。完整代码内容较多，请阅读源码查看。\n",
    "\n",
    "代码包括：\n",
    "- ./Model/common_layer.py\n",
    "- ./Model/EmoPrepend.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 1. 通用模块封装（common_layer.py）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在 `common_layer.py` 中实现了 Transformer 及相关通用模块的核心文件，主要包括以下几大部分：\n",
    "\n",
    "1. 全局初始化与依赖  \n",
    "2. EncoderLayer / GraphLayer / DecoderLayer  \n",
    "3. MultiHeadAttention (多头注意力)  \n",
    "4. Conv 与 PositionwiseFeedForward\n",
    "5. LayerNorm  \n",
    "6. 掩码与位置编码工具函数  \n",
    "7. Embedding 相关  \n",
    "8. LabelSmoothing  \n",
    "9. NoamOpt（学习率调度器）\n",
    "\n",
    "以下文档中仅展示最核心的代码模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as I\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 多头注意力机制\n",
    "MultiHeadAttention (多头注意力) 包含：\n",
    "   - 线性映射：query/key/value → 划分多头 → 缩放点积 → Mask（可选）  \n",
    "   - Softmax 归一化 → Dropout → 拼回各头输出 → 线性投影  \n",
    "   - 返回 `(outputs, attention_weights)`，其中 `attention_weights` 是跨头平均后的注意力权重  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_depth, total_key_depth, total_value_depth, output_depth,\n",
    "                 num_heads, bias_mask=None, dropout=0.0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            input_depth (int): 输入张量最后一维维度\n",
    "            total_key_depth (int): 键张量最后一维维度，需被 num_heads 整除\n",
    "            total_value_depth (int): 值张量最后一维维度，需被 num_heads 整除\n",
    "            output_depth (int): 输出张量最后一维维度\n",
    "            num_heads (int): 注意力头数量\n",
    "            bias_mask (Tensor or None): 用于屏蔽未来位置的掩码\n",
    "            dropout (float): 注意力权重上的 Dropout 概率（训练时非零）\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        if total_key_depth % num_heads != 0:\n",
    "            print(\"Key depth (%d) must be divisible by the number of \"\n",
    "                  \"attention heads (%d).\" % (total_key_depth, num_heads))\n",
    "            total_key_depth = total_key_depth - (total_key_depth % num_heads)\n",
    "        if total_value_depth % num_heads != 0:\n",
    "            print(\"Value depth (%d) must be divisible by the number of \"\n",
    "                  \"attention heads (%d).\" % (total_value_depth, num_heads))\n",
    "            total_value_depth = total_value_depth - (total_value_depth % num_heads)\n",
    "        self.num_heads = num_heads\n",
    "        self.query_scale = (total_key_depth // num_heads) ** -0.5\n",
    "        self.bias_mask = bias_mask\n",
    "        self.query_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n",
    "        self.key_linear = nn.Linear(input_depth, total_key_depth, bias=False)\n",
    "        self.value_linear = nn.Linear(input_depth, total_value_depth, bias=False)\n",
    "        self.output_linear = nn.Linear(total_value_depth, output_depth, bias=False)\n",
    "        self.emotion_output_linear = nn.Linear(2 * output_depth, output_depth, bias=False)\n",
    "        self.W_vad = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"\n",
    "        拆分多头：在特征维度上增加 num_heads 维度\n",
    "        输入:\n",
    "            x: 形状为 [batch_size, seq_length, depth] 的张量\n",
    "        返回:\n",
    "            形状为 [batch_size, num_heads, seq_length, depth/num_heads] 的张量\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 3:\n",
    "            raise ValueError(\"x must have rank 3\")\n",
    "        shape = x.shape\n",
    "        return x.view(shape[0], shape[1], self.num_heads, shape[2] // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "    def _merge_heads(self, x):\n",
    "        \"\"\"\n",
    "        合并多头：将 num_heads 维度合并回最后一维\n",
    "        输入:\n",
    "            x: 形状为 [batch_size, num_heads, seq_length, depth/num_heads] 的张量\n",
    "        返回:\n",
    "            形状为 [batch_size, seq_length, depth] 的张量\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(\"x must have rank 4\")\n",
    "        shape = x.shape\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(shape[0], shape[2], shape[3] * self.num_heads)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask):\n",
    "        # 对查询、键、值分别进行线性映射\n",
    "        queries = self.query_linear(queries)\n",
    "        keys = self.key_linear(keys)\n",
    "        values = self.value_linear(values)\n",
    "\n",
    "        # 拆分多头\n",
    "        queries = self._split_heads(queries)  # (bsz, heads, len, key_depth-20)\n",
    "        keys = self._split_heads(keys)\n",
    "        values = self._split_heads(values)\n",
    "\n",
    "        # 缩放查询向量\n",
    "        queries *= self.query_scale\n",
    "        # 计算注意力得分\n",
    "        logits = torch.matmul(queries, keys.permute(0, 1, 3, 2))  # (bsz, head, tgt_len, src_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # 扩展掩码维度至 [B, 1, 1, T_values]\n",
    "            logits = logits.masked_fill(mask, -1e18)\n",
    "\n",
    "        # 计算整体注意力权重 (平均各头)\n",
    "        attetion_weights = logits.sum(dim=1) / self.num_heads  # (bsz, tgt_len, src_len)\n",
    "        # 将得分转换为概率分布\n",
    "        weights = nn.functional.softmax(logits, dim=-1)  # (bsz, 2, tgt_len, src_len)\n",
    "        # 对注意力概率进行 Dropout\n",
    "        weights = self.dropout(weights)\n",
    "        # 与值向量相乘，获取上下文表示\n",
    "        contexts = torch.matmul(weights, values)\n",
    "        # 合并多头，恢复上下文向量形状\n",
    "        contexts = self._merge_heads(contexts)\n",
    "        # contexts = torch.tanh(contexts)\n",
    "        # 线性变换输出\n",
    "        outputs = self.output_linear(contexts)  # 50 -> 300\n",
    "        # 返回输出及注意力权重（已归一化）\n",
    "        return outputs, torch.softmax(attetion_weights, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 编码器层\n",
    "\n",
    "EncoderLayer 包含：\n",
    "- 自注意力（MultiHeadAttention）+ 前馈网络（PositionwiseFeedForward）  \n",
    "- 每个子层前都做 LayerNorm，子层输出后做残差连接和 Dropout  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                                dataset: empathetic                             \n",
      "                             hidden_dim: 300                                    \n",
      "                                emb_dim: 300                                    \n",
      "                             batch_size: 16                                     \n",
      "                                 epochs: 100                                    \n",
      "                                     lr: 0.0001                                 \n",
      "                          max_grad_norm: 2.0                                    \n",
      "                              beam_size: 5                                      \n",
      "                              save_path: results/tb_results/test/               \n",
      "                      save_path_dataset: results/tb_results/                    \n",
      "                            resume_path: result/                                \n",
      "                              device_id: 0                                      \n",
      "                                dropout: 0.2                                    \n",
      "                            pointer_gen: 1                                      \n",
      "                          teacher_ratio: 1.0                                    \n",
      "                           pretrain_emb: 1                                      \n",
      "                                  model: EmpDG                                  \n",
      "                        label_smoothing: 1                                      \n",
      "                                   noam: 1                                      \n",
      "                        act_loss_weight: 0.001                                  \n",
      "                                    hop: 1                                      \n",
      "                                  heads: 2                                      \n",
      "                                  depth: 40                                     \n",
      "                                 filter: 50                                     \n",
      "                              gp_lambda: 0.1                                    \n",
      "                         rnn_hidden_dim: 300                                    \n",
      "                                d_steps: 1                                      \n",
      "                                g_steps: 5                                      \n",
      "                          adver_itr_num: 5000                                   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from Model.common_layer import Conv, PositionwiseFeedForward, LayerNorm, MultiHeadAttention\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 编码器（Encoder）中的单层结构。\n",
    "    参考论文：https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n",
    "                 bias_mask=None, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            hidden_size (int): 隐藏层维度大小\n",
    "            total_key_depth (int): 键（Key）张量最后一维的大小，需能被 num_heads 整除\n",
    "            total_value_depth (int): 值（Value）张量最后一维的大小，需能被 num_heads 整除\n",
    "            output_depth (int): 输出张量最后一维的大小\n",
    "            filter_size (int): 前馈网络（FFN）中间隐藏层的大小\n",
    "            num_heads (int): 多头注意力头数\n",
    "            bias_mask (Tensor or None): 用于屏蔽未来位置的掩码，防止信息泄露\n",
    "            layer_dropout (float): 本层残差连接后的 Dropout 概率\n",
    "            attention_dropout (float): 注意力权重上的 Dropout 概率（训练时非零）\n",
    "            relu_dropout (float): 前馈网络中 ReLU 后的 Dropout 概率（训练时非零）\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth,\n",
    "                                                       hidden_size, num_heads, bias_mask, attention_dropout)\n",
    "        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n",
    "                                                                 layer_config='cc', padding='both',\n",
    "                                                                 dropout=relu_dropout)\n",
    "        self.dropout = nn.Dropout(layer_dropout)\n",
    "        self.layer_norm_mha = LayerNorm(hidden_size)\n",
    "        self.layer_norm_ffn = LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        # 层归一化：对输入进行标准化处理，稳定模型训练\n",
    "        x_norm = self.layer_norm_mha(x)\n",
    "        # 多头注意力：执行自注意力计算，融合全局信息\n",
    "        y, _ = self.multi_head_attention(x_norm, x_norm, x_norm, mask)\n",
    "        # 残差连接并进行 Dropout\n",
    "        x = self.dropout(x + y)\n",
    "        # 层归一化：对残差输出进行标准化\n",
    "        x_norm = self.layer_norm_ffn(x)\n",
    "        # 前馈网络：逐位置进行两层线性变换及激活\n",
    "        y = self.positionwise_feed_forward(x_norm)\n",
    "        # 残差连接并进行 Dropout\n",
    "        y = self.dropout(x + y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 图网络层\n",
    "\n",
    "GraphLayer中\n",
    "- 只包含“编码—解码”注意力，用于将编码器输出与图结构信息融合  \n",
    "- 同样使用 LayerNorm → 注意力 → 残差 → LayerNorm → 前馈 → 残差\n",
    "\n",
    "encoder_outputs 中通常已经包含了对话上下文及情感图谱的特征，GraphLayer 通过跨注意力将这些图结构／情感信息融合到解码器每一步的表示里，起到图神经网络层（Graph Neural Network Layer）的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 解码器（Decoder）中的图网络层（GraphLayer）。\n",
    "    参考论文：https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n",
    "                 bias_mask, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            hidden_size (int): 隐藏层维度\n",
    "            total_key_depth (int): Key 张量最后一维维度，需被 num_heads 整除\n",
    "            total_value_depth (int): Value 张量最后一维维度，需被 num_heads 整除\n",
    "            filter_size (int): 前馈网络中间层维度\n",
    "            num_heads (int): 注意力头数量\n",
    "            bias_mask (Tensor): 屏蔽未来位置的掩码\n",
    "            layer_dropout (float): 本层残差后的 Dropout 概率\n",
    "            attention_dropout (float): 注意力权重上的 Dropout 概率\n",
    "            relu_dropout (float): 前馈网络中 ReLU 后的 Dropout 概率\n",
    "        \"\"\"\n",
    "\n",
    "        super(GraphLayer, self).__init__()\n",
    "        self.multi_head_attention_enc_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth,\n",
    "                                                               hidden_size, num_heads, None, attention_dropout)\n",
    "        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n",
    "                                                                 layer_config='cc', padding='left',\n",
    "                                                                 dropout=relu_dropout)\n",
    "        self.dropout = nn.Dropout(layer_dropout)\n",
    "        self.layer_norm_mha_dec = LayerNorm(hidden_size)\n",
    "        self.layer_norm_mha_enc = LayerNorm(hidden_size)\n",
    "        self.layer_norm_ffn = LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        注：inputs 为元组，包含解码器输入、编码器输出、注意力权重、源掩码\n",
    "        \"\"\"\n",
    "        x, encoder_outputs, attention_weight, mask_src = inputs\n",
    "\n",
    "        # 在进行编码器-解码器注意力前先做层归一化，稳定训练\n",
    "        x_norm = self.layer_norm_mha_enc(x)\n",
    "        # 多头编码器-解码器注意力，融合编码器信息\n",
    "        y, attention_weight = self.multi_head_attention_enc_dec(x_norm, encoder_outputs, encoder_outputs, mask_src)\n",
    "        # 编码器-解码器注意力后的残差连接并 Dropout\n",
    "        x = self.dropout(x + y)\n",
    "        # 层归一化\n",
    "        x_norm = self.layer_norm_ffn(x)\n",
    "        # 前馈网络：逐位置线性 + 激活 + 线性\n",
    "        y = self.positionwise_feed_forward(x_norm)\n",
    "        # 前馈网络后的残差连接并 Dropout\n",
    "        y = self.dropout(x + y)\n",
    "        return y, encoder_outputs, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 解码器层\n",
    "\n",
    "DecoderLayer中包含\n",
    "- 标准 Transformer 解码器层：先做掩码自注意力，再做编码—解码注意力，最后做前馈网络\n",
    "- 每步均包含 LayerNorm、残差连接和 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    对序列中的每个位置先后执行线性变换 + ReLU + 线性变换\n",
    "    \"\"\"\n",
    "    def __init__(self, input_depth, filter_size, output_depth, layer_config='ll', padding='left', dropout=0.0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            input_depth (int): 输入张量最后一维维度\n",
    "            filter_size (int): 前馈网络中间层维度\n",
    "            output_depth (int): 输出张量最后一维维度\n",
    "            layer_config (str): 'll' -> 全线性结构；'cc' -> 卷积结构\n",
    "            padding (str): 'left' -> 在左侧填充；'both' -> 在两侧填充\n",
    "            dropout (float): 前馈网络中 Dropout 概率（训练时非零）\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        layers = []\n",
    "        sizes = ([(input_depth, filter_size)] +\n",
    "                 [(filter_size, filter_size)] * (len(layer_config) - 2) +\n",
    "                 [(filter_size, output_depth)])\n",
    "        for lc, s in zip(list(layer_config), sizes):\n",
    "            if lc == 'l':\n",
    "                layers.append(nn.Linear(*s))\n",
    "            elif lc == 'c':\n",
    "                layers.append(Conv(*s, kernel_size=3, pad_type=padding))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown layer type {}\".format(lc))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers):\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer 解码器（Decoder）中的单层结构。\n",
    "    参考论文：https://arxiv.org/pdf/1706.03762.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, total_key_depth, total_value_depth, filter_size, num_heads,\n",
    "                 bias_mask, layer_dropout=0.0, attention_dropout=0.0, relu_dropout=0.0):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            hidden_size (int): 隐藏层维度\n",
    "            total_key_depth (int): Key 张量最后一维维度，需被 num_heads 整除\n",
    "            total_value_depth (int): Value 张量最后一维维度，需被 num_heads 整除\n",
    "            filter_size (int): 前馈网络中间层维度\n",
    "            num_heads (int): 注意力头数量\n",
    "            bias_mask (Tensor): 屏蔽未来位置的掩码\n",
    "            layer_dropout (float): 本层残差后的 Dropout 概率\n",
    "            attention_dropout (float): 注意力权重上的 Dropout 概率\n",
    "            relu_dropout (float): 前馈网络中 ReLU 后的 Dropout 概率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.multi_head_attention_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth,\n",
    "                                                           hidden_size, num_heads, bias_mask, attention_dropout)\n",
    "        self.multi_head_attention_enc_dec = MultiHeadAttention(hidden_size, total_key_depth, total_value_depth,\n",
    "                                                               hidden_size, num_heads, None, attention_dropout)\n",
    "        self.positionwise_feed_forward = PositionwiseFeedForward(hidden_size, filter_size, hidden_size,\n",
    "                                                                 layer_config='cc', padding='left',\n",
    "                                                                 dropout=relu_dropout)\n",
    "        self.dropout = nn.Dropout(layer_dropout)\n",
    "        self.layer_norm_mha_dec = LayerNorm(hidden_size)\n",
    "        self.layer_norm_mha_enc = LayerNorm(hidden_size)\n",
    "        self.layer_norm_ffn = LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        注：inputs 为元组，包含解码器输入、编码器输出、注意力权重、源掩码\n",
    "        \"\"\"\n",
    "        x, encoder_outputs, attention_weight, mask = inputs\n",
    "        mask_src, dec_mask = mask\n",
    "        # 在进行解码器自注意力前先做层归一化，稳定训练\n",
    "        x_norm = self.layer_norm_mha_dec(x)\n",
    "        # 掩码多头自注意力：对自身序列进行注意力计算，防止未来信息泄露\n",
    "        y, _ = self.multi_head_attention_dec(x_norm, x_norm, x_norm, dec_mask)\n",
    "        # 自注意力后的残差连接并 Dropout\n",
    "        x = self.dropout(x + y)\n",
    "        # 在进行编码器-解码器注意力前先做层归一化，稳定训练\n",
    "        x_norm = self.layer_norm_mha_enc(x)\n",
    "        # 多头编码器-解码器注意力，融合编码器输出\n",
    "        y, attention_weight = self.multi_head_attention_enc_dec(x_norm, encoder_outputs, encoder_outputs, mask_src)\n",
    "        # 编码器-解码器注意力后的残差连接并 Dropout\n",
    "        x = self.dropout(x + y)\n",
    "        # 层归一化\n",
    "        x_norm = self.layer_norm_ffn(x)\n",
    "        # 前馈网络：逐位置两个线性变换及激活\n",
    "        y = self.positionwise_feed_forward(x_norm)\n",
    "        # 前馈网络后的残差连接并 Dropout\n",
    "        y = self.dropout(x + y)\n",
    "        return y, encoder_outputs, attention_weight, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 共情对话模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，将详细介绍为适应本项目的共情对话场景，应该如何对原始 Transformer 模型进行改动，使得 Transformer 模型可以「识别情感⇢表达情感⇢生成共情回复」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 原始 Transformer 回顾  \n",
    "\n",
    "1. Embedding + Positional Encoding  \n",
    "   - 对每个输入 token 转为向量 $e_i\\in\\mathbb R^d$，再加上正余弦位置编码 $\\mathrm{PE}(i)$：  \n",
    "   $$x_i = e_i + \\mathrm{PE}(i)$$\n",
    "   - 代码：  \n",
    "      ```python\n",
    "      x = self.input_dropout(inputs)\n",
    "      x = self.embedding_proj(x)\n",
    "      x += self.timing_signal[:, :seq_len, :]\n",
    "      ```  \n",
    "2. 多头自注意力 (Self-Attention)  \n",
    "   - 对序列 ${x_i}$ 计算：  \n",
    "     $$\\mathrm{Attn}(Q,K,V)=\\mathrm{softmax}\\bigl(\\tfrac{QK^T}{\\sqrt{d_k}}+\\mathrm{Mask}\\bigr)V$$\n",
    "   - MultiHead 会并行执行 $h$ 次，再拼接、线性映射回维度 $d$。  \n",
    "3. 前馈网络 (FFN)  \n",
    "   - 对每个位置独立执行  \n",
    "     $\\mathrm{FFN}(x)=\\max(0, xW_1+b_1)W_2+b_2.$  \n",
    "4. Encoder×N 层 与 Decoder×N 层  \n",
    "   - Encoder 层堆叠自注意力 + FFN；Decoder 第一层做因果屏蔽自注意力，第二层做跨注意力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 情感预置\n",
    "\n",
    "我们通过采用共享设计的词嵌入，并使用额外的“情感掩码嵌入”与上下文嵌入相加，实现“情感前置/预置”的效果。\n",
    "\n",
    "`NRCDict.json` 是存放在 `datasets/empathetic-dialogue/` 下的一个外部情感词典，其内容是一个 JSON 数组，第一项（`[0]`）即是一个情感词汇表列表。模型在启动时会加载该情感词典：\n",
    "\n",
    "```python\n",
    "EMODICT = json.load(open('datasets/empathetic-dialogue/NRCDict.json'))[0]\n",
    "```\n",
    "\n",
    "然后，构造情感提示 ID 序列 `mask_context`，对数据集上下文中的每个词，若在 EMODICT 中，则对应位置设置一个「提示 ID」（>0），否则为 0。\n",
    "\n",
    "通过叠加情感提示嵌入，对特定「情感词」位置注入可学习偏置，引导后续注意力与表示把这些位置看得更重要。\n",
    "$$\n",
    "\\underbrace{\\mathrm{emb}(c_i)}_{\\text{上下文嵌入}} \\;+\\;\n",
    "\\underbrace{\\mathrm{emb}(\\mathrm{mask\\_context}_i)}_{\\text{情感提示嵌入}}\n",
    "\\;\\to\\; x_i\n",
    "$$\n",
    "\n",
    "代码：\n",
    "```python\n",
    "# 取自 baselines/EmoPrepend.py 的核心调用思路\n",
    "self.embedding = share_embedding(self.vocab, config.pretrain_emb)\n",
    "...\n",
    "mask_src = enc_batch.data.eq(config.PAD_idx).unsqueeze(1)  # (bsz, 1, src_len)\n",
    "emb_mask = self.embedding(batch[\"mask_context\"])         # 由外部构造的情感提示 ids → 嵌入\n",
    "src_emb = self.embedding(enc_batch) + emb_mask            # 叠加到上下文嵌入上\n",
    "encoder_outputs = self.encoder(src_emb, mask_src)\n",
    "```\n",
    "\n",
    "要点：\n",
    "- `share_embedding`：提供共享的 `nn.Embedding`（可与生成头权重共享）。\n",
    "- `mask_context`：与 `enc_batch` 等长的“提示序列”，在需要强调的 token 位置（如情感词、线索词）注入可学习的偏置，形成“情感引导”。\n",
    "- `src_emb = embedding(context) + embedding(mask)`：简单相加即可在编码层面实现引导，无需改动注意力形态。\n",
    "\n",
    "此外，在训练或分析时，也会用 get_emotion_words 提取生成句中的情感关键词：\n",
    "```python\n",
    "def get_emotion_words(utt_words):\n",
    "    \"\"\"\n",
    "    从情感词典中获取与输入词列表匹配的情感词\n",
    "    参数:\n",
    "        utt_words (list[str]): 分词后的句子词列表\n",
    "    返回:\n",
    "        emo_ws (list[str]): 匹配到的情感词列表\n",
    "    \"\"\"\n",
    "    emo_ws = []\n",
    "    for u in utt_words:\n",
    "        if u in EMODICT:\n",
    "            emo_ws.append(u)\n",
    "    return emo_ws\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 情感分类头\n",
    "\n",
    "1. 全局情感特征提取  \n",
    "   - 类似 BERT 的 [CLS]：取 `encoder_outputs[:,0]` 作为整句聚合向量 $q_h$。  \n",
    "2. 线性映射到情感类别  \n",
    "    $$\n",
    "    \\ell = W_{\\text{emo}}\\,q_h + b_{\\text{emo}},\\quad\n",
    "    p(\\text{emo})=\\mathrm{softmax}(\\ell)\n",
    "    $$ \n",
    "    代码：\n",
    "\n",
    "    ```python\n",
    "    q_h = encoder_outputs[:, 0]\n",
    "    emotion_logit = self.decoder_key(q_h)\n",
    "    loss_emotion = nn.CrossEntropyLoss(reduction='sum')(emotion_logit, batch['emotion_label'])\n",
    "    pred_emotion = np.argmax(emotion_logit.detach().cpu().numpy(), axis=1)\n",
    "    ...\n",
    "    loss = self.criterion(logit.contiguous().view(-1, logit.size(-1)), dec_batch.contiguous().view(-1))\n",
    "    loss += loss_emotion\n",
    "    loss += loss_from_d\n",
    "    ```\n",
    "3. 多任务联合训练\n",
    "    $$\n",
    "    \\mathcal{L}=\\mathcal{L}_{\\mathrm{gen}}+\\lambda_{\\mathrm{emo}}\\mathcal{L}_{\\mathrm{emotion}}\n",
    "    \\quad(\\,+\\mathcal{L}_{\\mathrm{adv}}\\;\\text{可选})\n",
    "    $$\n",
    "    - 使编码器既要学习生成信号，也要捕捉情感信号；梯度同时来自生成与分类，增强情感感知。\n",
    "    - 情感辅助任务督促编码器关注情感信号，与“情感预置”的引导相互补充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 指针-生成器\n",
    "\n",
    "为兼顾“复制源文本片段”和“生成词表内词”，`EmoPrepend` 使用门控 `p_gen` 在两种分布间进行加权并融合到扩展词表：\n",
    "\n",
    "1. 生成分布 vs. 复制分布  \n",
    "    - 生成门：  \n",
    "    $$p_{\\text{gen}}=\\sigma(w_p^T x_t + b_p),\\quad \\alpha_t=p_{\\text{gen}}$$  \n",
    "    - 常规词表分布  \n",
    "    $$P_{\\mathrm{vocab}}=\\mathrm{softmax}(W_o x_t / T)$$  \n",
    "    - 注意力复制分布  \n",
    "    $$P_{\\mathrm{copy}} = \\mathrm{softmax}(\\mathrm{attn\\_scores})$$  \n",
    "2. 融合\n",
    "    $$\n",
    "    P(w)=\\alpha_t\\,P_{\\mathrm{vocab}}(w)\\;\n",
    "        +\\;(1-\\alpha_t)\\sum_{i:x_i=w}P_{\\mathrm{copy}}(i).\n",
    "    $$\n",
    "\n",
    "代码：\n",
    "```python\n",
    "# 线性变换得到生成门（与上下文有关）\n",
    "p_gen = torch.sigmoid(self.p_gen_linear(x))              # (B, T, 1)\n",
    "\n",
    "# 常规词表分布与注意力分布（经 softmax 温度）\n",
    "vocab_dist = softmax(Wx/temp)                            # (B, T, V)\n",
    "attn_dist  = softmax(attn/temp)                          # (B, T, S)\n",
    "\n",
    "# 门控融合\n",
    "a = p_gen * vocab_dist                                   # 生成部分\n",
    "b = (1 - p_gen) * attn_dist                              # 复制部分\n",
    "\n",
    "# 将注意力分布散射到“扩展词表”索引位置（OOV 统一映射到扩展位置）\n",
    "logit = log(a.scatter_add(2, enc_batch_extend_vocab, b))\n",
    "```\n",
    "\n",
    "要点：\n",
    "- `enc_batch_extend_vocab`：把每个源 token 在“扩展词表”中的索引提供给 `scatter_add`，把注意力概率累加到正确的词位。\n",
    "- 复制能力对于 OOV、专有名词、长尾表达尤为关键，是对话与摘要类任务常用增强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 训练细节与学习率调度  \n",
    "1. Label Smoothing（可选）  \n",
    "   $$\n",
    "   \\mathcal{L}_{\\mathrm{LS}}=-\\sum (1-\\epsilon)y\\log p+( \\epsilon/(V-1))\\sum_{y'\\neq y}\\log p(y').\n",
    "   $$\n",
    "2. NoamOpt 学习率策略  \n",
    "   $$\n",
    "   \\mathrm{lr} \\propto d^{-0.5}\\min(\\text{step}^{-0.5},\\;\\text{step}\\times \\mathrm{warmup}^{-1.5}).\n",
    "   $$\n",
    "   - 保证前期快速上升，后期平稳收敛。\n",
    "\n",
    "\n",
    "通过上面几个模块的无缝对接，EmoPrepend 在输入端主动「标记情感」、中间端以「分类头」监督情感语义、输出端以「指针-生成」复合分布，最终让模型既能正确捕捉对话情感，又能在生成时自如复现、共情反馈。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以实例化完整的模型来输出查看模型的结构和参数统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型创建完成!\n",
      "\n",
      "============================================================\n",
      "EmoPrepend 结构摘要\n",
      "============================================================\n",
      "参数总量: 3,049,519  |  可训练: 3,049,519\n",
      "词汇表: 10000  |  嵌入维度: 100\n",
      "编码层数: 6  |  隐藏维度: 100  |  头数: 1\n",
      "情感类别数: 32\n",
      "优化器: Adam(lr=0.0001)\n",
      "------------------------------------------------------------\n",
      "参数分布（按主要模块）\n",
      "- Embedding:   1,000,000\n",
      "- Encoder:     409,506\n",
      "- Decoder:     626,712\n",
      "- Generator:   1,010,101\n",
      "- EmotionHead: 3,200\n",
      "------------------------------------------------------------\n",
      "结构摘要（首层模板）\n",
      "- Embedding 权重: (10000, 100)\n",
      "- Encoder 层数: 6\n",
      "  Encoder 单层模板:\n",
      "  - EncoderLayer\n",
      "    子模块: multi_head_attention, positionwise_feed_forward, dropout, layer_norm_mha, layer_norm_ffn\n",
      "- Decoder 层数: 6\n",
      "  Decoder 单层模板:\n",
      "  - DecoderLayer\n",
      "    子模块: multi_head_attention_dec, multi_head_attention_enc_dec, positionwise_feed_forward, dropout, layer_norm_mha_dec, layer_norm_mha_enc, layer_norm_ffn\n",
      "- Generator: Linear(100 -> 10000)\n",
      "- EmotionHead: Linear(100 -> 32)\n",
      "device=cpu, emb_dim=100, hidden_dim=100, hop=6, heads=1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if hasattr(sys, 'argv') and any('kernel' in arg for arg in sys.argv):\n",
    "    # 在Jupyter环境中，清理sys.argv以避免argparse冲突\n",
    "    original_argv = sys.argv.copy()\n",
    "    sys.argv = [sys.argv[0]]  # 只保留脚本名称\n",
    "\n",
    "from utils.data_loader import prepare_data_seq\n",
    "from utils.common import *\n",
    "from Model.EmoPrepend import EmoP\n",
    "from utils import config\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"统计模型参数数量\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def print_model_structure(model, show_templates=True):\n",
    "    \"\"\"打印适中详尽的模型结构摘要，兼顾信息量与输出长度\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EmoPrepend 结构摘要\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "    print(f\"参数总量: {total_params:,}  |  可训练: {trainable_params:,}\")\n",
    "    print(f\"词汇表: {model.vocab_size}  |  嵌入维度: {config.emb_dim}\")\n",
    "    print(f\"编码层数: {config.hop}  |  隐藏维度: {config.hidden_dim}  |  头数: {config.heads}\")\n",
    "    print(f\"情感类别数: {model.decoder_key.out_features}\")\n",
    "\n",
    "    flags = []\n",
    "    if getattr(config, 'pointer_gen', False):\n",
    "        flags.append(\"Pointer-Gen\")\n",
    "    if getattr(config, 'weight_sharing', False):\n",
    "        flags.append(\"WeightSharing\")\n",
    "    if getattr(config, 'noam', False):\n",
    "        flags.append(\"NoamLR\")\n",
    "\n",
    "    if getattr(config, 'noam', False):\n",
    "        optim_str = \"Noam 调度器\"\n",
    "    else:\n",
    "        optim_str = f\"Adam(lr={getattr(config, 'lr', 'N/A')})\"\n",
    "    print(f\"优化器: {optim_str}\")\n",
    "\n",
    "    def module_params(m):\n",
    "        return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(\"参数分布（按主要模块）\")\n",
    "    print(f\"- Embedding:   {module_params(model.embedding):,}\")\n",
    "    print(f\"- Encoder:     {module_params(model.encoder):,}\")\n",
    "    print(f\"- Decoder:     {module_params(model.decoder):,}\")\n",
    "    print(f\"- Generator:   {module_params(model.generator):,}\")\n",
    "    print(f\"- EmotionHead: {module_params(model.decoder_key):,}\")\n",
    "\n",
    "    if show_templates:\n",
    "        print(\"-\" * 60)\n",
    "        print(\"结构摘要（首层模板）\")\n",
    "        # Embedding\n",
    "        try:\n",
    "            emb_shape = tuple(model.embedding.lut.weight.shape)\n",
    "            print(f\"- Embedding 权重: {emb_shape}\")\n",
    "        except Exception:\n",
    "            print(\"- Embedding 权重: 未知\")\n",
    "\n",
    "        # Encoder\n",
    "        print(f\"- Encoder 层数: {getattr(config, 'hop', 'N/A')}\")\n",
    "        enc0 = None\n",
    "        try:\n",
    "            enc0 = model.encoder.enc[0]\n",
    "        except Exception:\n",
    "            try:\n",
    "                enc0 = model.encoder.enc\n",
    "            except Exception:\n",
    "                enc0 = None\n",
    "        if enc0 is not None:\n",
    "            print(\"  Encoder 单层模板:\")\n",
    "            child_names = [n for n, _ in enc0.named_children()]\n",
    "            print(f\"  - {enc0.__class__.__name__}\")\n",
    "            print(f\"    子模块: {', '.join(child_names)}\")\n",
    "        else:\n",
    "            print(\"  Encoder 单层模板: 未获取\")\n",
    "\n",
    "        # Decoder\n",
    "        try:\n",
    "            dec0 = model.decoder.dec[0]\n",
    "        except Exception:\n",
    "            try:\n",
    "                dec0 = next(iter(model.decoder.dec))\n",
    "            except Exception:\n",
    "                dec0 = None\n",
    "        print(f\"- Decoder 层数: {getattr(config, 'hop', 'N/A')}\")\n",
    "        if dec0 is not None:\n",
    "            print(\"  Decoder 单层模板:\")\n",
    "            child_names = [n for n, _ in dec0.named_children()]\n",
    "            print(f\"  - {dec0.__class__.__name__}\")\n",
    "            print(f\"    子模块: {', '.join(child_names)}\")\n",
    "        else:\n",
    "            print(\"  Decoder 单层模板: 未获取\")\n",
    "\n",
    "        # Generator & Emotion head\n",
    "        print(f\"- Generator: Linear({getattr(config, 'hidden_dim', 'H')} -> {model.vocab_size})\")\n",
    "        print(f\"- EmotionHead: Linear({getattr(config, 'hidden_dim', 'H')} -> {model.decoder_key.out_features})\")\n",
    "\n",
    "def create_emoprepend_model():\n",
    "    \"\"\"创建并返回EmoPrepend模型\"\"\"\n",
    "    class SimpleVocab:\n",
    "        def __init__(self):\n",
    "            self.n_words = 10000\n",
    "            self.word2index = {}\n",
    "            self.index2word = {}\n",
    "        \n",
    "    vocab = SimpleVocab()\n",
    "    program_number = 32  # 默认情感类别数\n",
    "    \n",
    "    # 实例化模型\n",
    "    model = EmoP(\n",
    "        vocab=vocab,                    # 词汇表对象\n",
    "        decoder_number=program_number,  # 情感类别数（32）\n",
    "        is_eval=False,                  # 是否为评估模式\n",
    "        load_optim=False                # 是否加载优化器状态\n",
    "    )\n",
    "    \n",
    "    # 移动到指定设备\n",
    "    model.to(config.device)\n",
    "    \n",
    "    print(\"模型创建完成!\")\n",
    "    print()\n",
    "    \n",
    "    return model, vocab, program_number\n",
    "\n",
    "\n",
    "model, vocab, program_number = create_emoprepend_model()\n",
    "\n",
    "# 输出模型结构\n",
    "print_model_structure(model)\n",
    "\n",
    "print(f\"device={getattr(config, 'device', 'cpu')}, emb_dim={getattr(config, 'emb_dim', 'N/A')}, hidden_dim={getattr(config, 'hidden_dim', 'N/A')}, hop={getattr(config, 'hop', 'N/A')}, heads={getattr(config, 'heads', 'N/A')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
