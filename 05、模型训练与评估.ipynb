{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章节将详细介绍本项目中各模型的训练方法和评估策略。\n",
    "\n",
    "模型类型：\n",
    "- **Transformer**: 基础的Transformer模型\n",
    "- **EmoPrepend**: 情感前置模型，将情感信息添加到输入前\n",
    "- **EmpDG_woD**: 无判别器的共情对话生成模型（仅生成器）\n",
    "- **EmpDG_woG**: 无生成器的模型（使用EmoPrepend作为生成器，仅训练判别器）\n",
    "- **EmpDG**: 完整的EmpDG模型（包含生成器和判别器的对抗训练）\n",
    "\n",
    "训练脚本：\n",
    "- `train.py`: 基础模型训练脚本\n",
    "- `adver_train.py`: 完整EmpDG模型的对抗训练脚本\n",
    "- `adver_train_no_eg.py`: 无多分辨率情感感知的对抗训练脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 模型训练\n",
    "\n",
    "#### 1.1 训练参数说明\n",
    "\n",
    "完整的参数请查看 `./utils/config.py`，这里仅展示最主要的参数设置。\n",
    "\n",
    "**核心参数：**\n",
    "| 参数 | 说明 | 默认值 | 备注 |\n",
    "|------|------|--------|------|\n",
    "| `--model` | 模型类型 | - | Transformer/EmoPrepend/EmpDG_woD/EmpDG/EmpDG_woG |\n",
    "| `--cuda` | 使用GPU | False | 建议开启以加速训练 |\n",
    "| `--device_id` | GPU设备ID | 0 | 指定使用的GPU |\n",
    "| `--batch_size` | 批次大小 | 16 | 根据GPU内存调整 |\n",
    "| `--epoch` | 最大训练轮数 | 100 | 有早停机制 |\n",
    "\n",
    "**模型结构参数：**\n",
    "| 参数 | 说明 | 默认值 | 备注 |\n",
    "|------|------|--------|------|\n",
    "| `--emb_dim` | 词嵌入维度 | 300 | 与GloVe向量维度保持一致 |\n",
    "| `--hidden_dim` | 隐藏层维度 | 300 | Transformer模型的隐藏维度 |\n",
    "| `--rnn_hidden_dim` | RNN隐藏维度 | 300 | 对于包含RNN的模型 |\n",
    "| `--hop` | Transformer层数 | 1 | 编码器/解码器层数 |\n",
    "| `--heads` | 多头注意力头数 | 2 | 注意力机制的头数 |\n",
    "\n",
    "**训练策略参数：**\n",
    "| 参数 | 说明 | 默认值 | 备注 |\n",
    "|------|------|--------|------|\n",
    "| `--noam` | Noam学习率调度 | False | 使用Transformer的学习率调度策略 |\n",
    "| `--label_smoothing` | 标签平滑 | False | 缓解过拟合 |\n",
    "| `--pretrain_emb` | 预训练词嵌入 | False | 使用GloVe词向量初始化 |\n",
    "| `--pointer_gen` | 指针生成网络 | False | 允许模型从输入中复制词汇 |\n",
    "\n",
    "**对抗训练参数：**\n",
    "| 参数 | 说明 | 默认值 | 备注 |\n",
    "|------|------|--------|------|\n",
    "| `--d_steps` | 判别器训练步数 | 1 | 每轮中判别器的更新次数 |\n",
    "| `--g_steps` | 生成器训练步数 | 5 | 每轮中生成器的更新次数 |\n",
    "| `--emotion_disc` | 情感判别器 | False | 是否使用情感判别器 |\n",
    "| `--resume_g` | 恢复生成器 | False | 从检查点恢复生成器训练 |\n",
    "| `--resume_d` | 恢复判别器 | False | 从检查点恢复判别器训练 |\n",
    "\n",
    "**其他参数：**\n",
    "| 参数 | 说明 | 默认值 | 备注 |\n",
    "|------|------|--------|------|\n",
    "| `--save_path` | 保存路径 | - | 模型权重和日志保存位置 |\n",
    "| `--test` | 测试模式 | False | 跳过训练，直接测试 |\n",
    "| `--specify_model` | 指定模型 | False | 使用特定模型路径进行测试 |\n",
    "| `--resume_path` | 恢复路径 | - | 指定模型的检查点路径 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 模型训练示例\n",
    "\n",
    "**模型训练命令示例：**\n",
    "1. Transformer基础模型\n",
    "    ```bash\n",
    "    python train.py --cuda --label_smoothing --noam --emb_dim 300 --hidden_dim 300 \\\n",
    "        --hop 1 --heads 2 --pretrain_emb --model Transformer --device_id 0 \\\n",
    "        --save_path results/tb_results/Transformer/ --pointer_gen\n",
    "    ```\n",
    "2. EmoPrepend模型\n",
    "    ```bash\n",
    "    python train.py --cuda --label_smoothing --noam --emb_dim 300 --hidden_dim 300 \\\n",
    "        --hop 1 --heads 2 --pretrain_emb --model EmoPrepend --device_id 0 \\\n",
    "        --save_path results/tb_results/EmoPrepend/ --pointer_gen\n",
    "    ```\n",
    "3. EmpDG_woD模型（无判别器的共情生成器）\n",
    "    ```bash\n",
    "    python train.py --cuda --label_smoothing --noam --emb_dim 300 --hidden_dim 300 \\\n",
    "        --hop 1 --heads 2 --pretrain_emb --model EmpDG_woD --device_id 0 \\\n",
    "        --save_path results/tb_results/EmpDG_woD/ --pointer_gen\n",
    "    ```\n",
    "4. EmpDG_woG模型（判别器，使用EmoPrepend作为生成器）\n",
    "    ```bash\n",
    "    python3 adver_train_no_eg.py --cuda --label_smoothing --noam --emb_dim 300 \\\n",
    "        --rnn_hidden_dim 300 --hidden_dim 300 --hop 1 --heads 2 --pretrain_emb \\\n",
    "        --model EmpDG_woG --device_id 0 --save_path results/tb_results/EmpDG_woG/ \\\n",
    "        --d_steps 1 --g_steps 5 --pointer_gen\n",
    "    ```\n",
    "5. EmpDG完整模型对抗训练\n",
    "    ```bash\n",
    "    python3 adver_train.py --cuda --label_smoothing --noam --emb_dim 300 \\\n",
    "        --rnn_hidden_dim 300 --hidden_dim 300 --hop 1 --heads 2 --emotion_disc \\\n",
    "        --pretrain_emb --model EmpDG --device_id 0 --save_path results/tb_results/EmpDG/ \\\n",
    "        --d_steps 1 --g_steps 5 --pointer_gen\n",
    "    ```\n",
    "6. 使用预训练模型继续训练\n",
    "    ```bash\n",
    "    python3 adver_train.py --cuda --label_smoothing --noam --emb_dim 300 \\\n",
    "        --rnn_hidden_dim 300 --hidden_dim 300 --hop 1 --heads 2 --emotion_disc \\\n",
    "        --pretrain_emb --model EmpDG --device_id 0 --save_path results/tb_results/EmpDG/ \\\n",
    "        --d_steps 1 --g_steps 5 --pointer_gen --resume_g --resume_d\n",
    "    ```\n",
    "\n",
    "对抗训练是本项目的核心特色，包含生成器和判别器的对抗学习过程。\n",
    "\n",
    "使用 adver_train.py 进行完整的EmpDG模型训练，包含以下三个阶段：\n",
    "\n",
    "阶段1：预训练共情生成器\n",
    "- 训练一个能够生成共情响应的生成器\n",
    "- 使用交叉熵损失和其他监督信号\n",
    "\n",
    "阶段2：预训练判别器\n",
    "- **语义判别器**: 区分生成响应和真实响应的语义质量\n",
    "- **情感判别器**: 区分生成响应和真实响应的情感适配性\n",
    "\n",
    "阶段3：联合对抗训练\n",
    "- 生成器和判别器进行对抗训练\n",
    "- 生成器试图\"欺骗\"判别器\n",
    "- 判别器试图\"识别\"生成的响应\n",
    "\n",
    "训练策略\n",
    "- **判别器步数(d_steps)**: 每轮训练中判别器的更新次数 (默认1次)\n",
    "- **生成器步数(g_steps)**: 每轮训练中生成器的更新次数 (默认5次)\n",
    "- **早停机制**: 基于验证集准确率进行早停"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们设置`epoch=1`来进行训练测试，以跑通整个算法流程（完整训练请训练上述完整模型训练的指令）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\python38.zip', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\DLLs', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg', '', 'C:\\\\Users\\\\惠普\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\win32', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\win32\\\\lib', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\Pythonwin', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\setuptools\\\\_vendor', '/apdcephfs/share_916081/qtli/install/ft_local/EmpDG']\n",
      "开始训练 EmoPrepend 模型...\n",
      "配置: emb_dim=300, hidden_dim=300, epochs=1\n",
      "加载数据...\n",
      "LOADING empathetic_dialogue ...\n",
      "[situation]: i spent all weekend working on my truck to fix a miss in the engine . despite spending over $ 200 in parts , it did not do a thing to fix the miss .\n",
      "[emotion]: angry\n",
      "[context]: ['i worked on my truck all weekend . spent $ 215 on parts and a special tool . still did not fix the miss in the engine .']\n",
      "[emotion context]: truck spent parts special fix miss engine\n",
      "[target]: what is the problem with the engine ?\n",
      "[feedback]: it has a miss . sometimes when you are driving it sputters and jerks and barely has any power . other times it drives just fine . i have replaced so many things on it and nothing makes a difference for very long .\n",
      " \n",
      "[situation]: a few years ago , my marriage broke up , and i found myself living alone for the first time in my life . though i eventually grew accustomed to the solitude , it took a while to get used to it .\n",
      "[emotion]: lonely\n",
      "[context]: ['i found myself divorced a few years ago , and for the first time in my life , i was living alone .']\n",
      "[emotion context]: found years ago first time life living\n",
      "[target]: i felt sad and depressed due to my insecurities and felt rejected from society\n",
      "[feedback]: yes , i felt that way for a while . but then i grew used to the solitude .\n",
      " \n",
      "[situation]: i felt proud when i had accomplished a dream that i wanted to do for a long time .\n",
      "[emotion]: proud\n",
      "[context]: ['i felt proud when i had accomplished a dream that i wanted to do for a long time . it was absailing ! !']\n",
      "[emotion context]: felt proud accomplished dream long time\n",
      "[target]: i am not familiar with abseiling , but i am glad that you felt good about it .\n",
      "[feedback]: absailing , its when you rappeling down a building . what have you down that amazing ?\n",
      " \n",
      "train length:  20724\n",
      "valid length:  2972\n",
      "test length:  2713\n",
      "创建模型...\n",
      "Embeddings: 22359 x 300\n",
      "Loading embedding file: vectors/glove.6B.300d.txt\n",
      "Pre-trained: 18064 (80.79%)\n",
      "MODEL USED EmoPrepend\n",
      "TRAINABLE PARAMETERS 14496563\n",
      "开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:4.6413; ppl:103.7: 100%|██████████| 186/186 [00:24<00:00,  7.61it/s]\n",
      "100%|██████████| 1296/1296 [05:58<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练完成!\n",
      "开始测试...\n",
      "testing generation (quiet mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:9.6987; ppl:16297.2: 100%|██████████| 2713/2713 [26:17<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试完成! Test Loss: 9.6987, PPL: 24725.8045, Acc: 0.0291; Dist1: 0.0068; Dist2: 0.0213\n",
      "{'training_completed': True, 'test_performed': True, 'loss': 9.698747108134668, 'ppl': 24725.804485522283, 'accuracy': 0.029119056395134537, 'best_val_ppl': 1000, 'dist1': 0.006772041658535881, 'dist2': 0.02128459197906955}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from run_train import train_notebook\n",
    "\n",
    "result = train_notebook(\n",
    "    epochs=1,\n",
    "    emb_dim=300,\n",
    "    hidden_dim=300,\n",
    "    hop=1,\n",
    "    heads=2,\n",
    "    pretrain_emb=True,\n",
    "    label_smoothing=True,\n",
    "    noam=True,\n",
    "    pointer_gen=True,\n",
    "    save_path=\"results/tb_results/EmoPrepend/\",\n",
    "    enable_test=True,      # 执行测试\n",
    "    verbose_test=False,    # 静默测试\n",
    "    model=\"EmoPrepend\",\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据 (adversarial)...\n",
      "LOADING empathetic_dialogue ...\n",
      "=====================STEP 1: Pre-train Empathetic Generator=====================\n",
      "Embeddings: 22359 x 300\n",
      "Loading embedding file: vectors/glove.6B.300d.txt\n",
      "Pre-trained: 18064 (80.79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Study\\ml_projects\\EmpDG-master\\adver_train.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('result/best_backup/EmpDG_woD_best.tar', map_location=lambda storage, location: storage)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================STEP 2: Pre-train Discriminators==========================\n",
      "Embeddings: 22359 x 300\n",
      "Loading embedding file: vectors/glove.6B.300d.txt\n",
      "Pre-trained: 18064 (80.79%)\n",
      "Embeddings: 22359 x 300\n",
      "Loading embedding file: vectors/glove.6B.300d.txt\n",
      "Pre-trained: 18064 (80.79%)\n",
      "=====================STEP 3: Adversarial joint learning=======================\n",
      "==================Test performance before adver train=====================\n",
      "LOADING empathetic_dialogue ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Study\\ml_projects\\EmpDG-master\\adver_train.py:210: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('result/best_backup/D_best.tar', map_location=lambda storage, location: storage)\n",
      "100%|██████████| 170/170 [08:04<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\tPPL\tAccuracy\tDist-1\tDist-2\n",
      "3.6748\t39.4426\t0.3349\t0.70\t6.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.7104; ppl:40.9: 100%|██████████| 186/186 [09:13<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "adver_training !\n",
      "EVAL\tLoss\tPPL\tAccuracy\tDist-1\tDist-2\n",
      "valid\t3.7104\t40.8687\t0.3491\t0.68\t6.70\n",
      "best_acc: 0.3491263440860215; patient: 0\n",
      "step 200 spend time: 557.036807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:3.8415; ppl:46.6: 100%|██████████| 186/186 [09:02<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "adver_training !\n",
      "EVAL\tLoss\tPPL\tAccuracy\tDist-1\tDist-2\n",
      "valid\t3.8415\t46.5954\t0.3150\t0.70\t6.06\n",
      "{'training_completed': True, 'test_performed': False}\n"
     ]
    }
   ],
   "source": [
    "from run_adver_train import adver_train_notebook\n",
    "\n",
    "result = adver_train_notebook(\n",
    "    epochs=1,\n",
    "    emb_dim=300,\n",
    "    hidden_dim=300,\n",
    "    hop=1,\n",
    "    heads=2,\n",
    "    pretrain_emb=True,\n",
    "    label_smoothing=True,\n",
    "    noam=True,\n",
    "    pointer_gen=True,\n",
    "    save_path=\"results/tb_results/EmpDG/\",\n",
    "    resume_g=True,\n",
    "    resume_d=True,\n",
    "    enable_test=False,      # 执行测试\n",
    "    verbose_test=False,    # 静默测试\n",
    "    model=\"EmpDG\",\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 2. 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里主要采用 **Perplexity、Distinct-1、Distinct-2、Emotion Accuracy** 这四个自动指标，对模型的性能进行综合性评估。\n",
    "\n",
    "#### 2.1 Perplexity（困惑度，PPL）\n",
    "\n",
    "**定义 / 公式**\n",
    "给定测试集中所有目标 token 的总数 $N$，模型按条件概率 $p(y_j\\mid y_{<j},C)$（通常用 teacher-forcing 的方式计算），困惑度定义为：\n",
    "\n",
    "$$\n",
    "\\mathrm{PPL} = \\exp\\Big(-\\frac{1}{N}\\sum_{j=1}^{N}\\log p(y_j\\mid y_{<j},C)\\Big).\n",
    "$$\n",
    "\n",
    "（也常写成 $\\mathrm{PPL} = \\exp(\\text{平均负对数似然})$）\n",
    "\n",
    "**实际计算要点**\n",
    "\n",
    "* 用**真值目标序列**来计算概率（即在每一步使用 gold 前缀，teacher-forcing），而不是用模型自己生成的序列来估概率。\n",
    "* 在实现上通常把 batch 内的对数概率累加后归一化到全数据上的负对数似然，再取指数。\n",
    "* 若你在训练时应用了 label-smoothing 或其他正则，要注意 PPL 的可比性：和 baseline 要一致设置。\n",
    "\n",
    "**EmpDG 中的含义 / 解读**\n",
    "\n",
    "* PPL 测量语言模型层面的“流畅性 / 模型对真实回复的拟合程度”。\n",
    "* 在 EmpDG 中，引入对抗判别器往往会 **提高多样性**（Distinct↑）但可能 **使模型的最大似然概率下降**，因此 PPL 反而变差（变大）。这并不一定意味着生成质量变差 —— 只是概率分布发生了变化（更分散、更多样）。论文中也观察到这种 trade-off。\n",
    "\n",
    "**优点**\n",
    "\n",
    "* 标准、可重复，衡量的是模型对训练/测试语料的概率拟合能力。\n",
    "\n",
    "**局限**\n",
    "\n",
    "* 对多模态回复（多个合理答案）不友好：真实的高质量但“少见”回复会提高 PPL。\n",
    "* 不能直接衡量回复的情感/共情性或多样性。\n",
    "\n",
    "#### 2.2 & 2.3 Distinct-1 / Distinct-2（表面多样性）\n",
    "\n",
    "**定义）**\n",
    "对整个测试集上所有**生成的回复**（不是 gold），统计不重复的 n-gram 数：\n",
    "\n",
    "$$\n",
    "\\text{Distinct-}n = \\frac{\\#\\ \\text{distinct n-grams in all outputs}}{\\#\\ \\text{total generated tokens (or total n-grams)} }.\n",
    "$$\n",
    "\n",
    "**实际计算要点**\n",
    "\n",
    "* 在计算时只统计 *模型生成的* 文本，不包括 gold。\n",
    "* 典型做法是把 test 集上所有生成回复拼在一起统计 distinct，得到 corpus-level distinct；也可做 sentence-level distinct 然后平均（两者不同，paper 通常用 corpus-level）。\n",
    "* 区分 unigram（n=1）和 bigram（n=2），分别报告 Distinct-1/2。\n",
    "\n",
    "**EmpDG 中的含义**\n",
    "\n",
    "* Distinct-1/2 衡量的是**表面 n-gram 多样性**：值越大说明生成回复更不“千篇一律”。\n",
    "* EmpDG 中对抗训练（interactive discriminators）鼓励生成器输出更“非典型”的回复，从而 Distinct-2 在论文里显著上涨，说明 bigram 级别的多样性增加（更丰富的短语组合）。\n",
    "* 但高 Distinct 并不自动等同于“好”的回复——可能生成更多不同但不相关或不连贯的短语，所以必须结合 Relevance/Empathy 人工评分解读。\n",
    "\n",
    "**优点**\n",
    "\n",
    "* 直观、能显著反映“通用回复”问题的缓解效果（higher ≈ less generic）。\n",
    "\n",
    "**局限**\n",
    "\n",
    "* 只测表面多样性：不能衡量语义多样性或情感适配性。\n",
    "* 对 rare token 的使用敏感：错误/拼写/长尾词也会提高 distinct。\n",
    "* 规范化方式（是否乘 100、除以 tokens 还是 n-grams）会影响绝对值，比较时要一致。\n",
    "\n",
    "#### 2.4 Emotion Accuracy（情感准确率）\n",
    "\n",
    "**论文中具体做法（EmpDG 的做法）**\n",
    "论文文本写法是：“Emotion Accuracy as the agreement between the ground truth emotion labels and the predicted emotion labels by the empathetic generator.”\n",
    "在 EmpDG 的模型结构中，情感预测向量 $\\mathbf{e}_p$ 是由 **情感 encoder（基于 context）** 预测得到的 coarse-grained 情感类别分布（公式 7、8）。因此 **Emotion Accuracy 在此处主要衡量模型从上下文识别并预测正确情感标签的能力（emotion recognition on context）**，即模型是否正确“理解”用户在上下文里表达的情绪类别。\n",
    "\n",
    "> 小结：**不是直接比较生成回复的情绪表达本身**，而是比较模型对 **对话情感标签的预测** 与数据集给定标签的一致性。\n",
    "\n",
    "**定义）**\n",
    "令 $e^\\ast$ 为数据集给定的对话级情感标签，令 $\\hat{e} = \\arg\\max P_e(e\\mid \\mathcal{E})$ 为模型预测的情感类别，则：\n",
    "\n",
    "$$\n",
    "\\text{EmotionAccuracy} = \\frac{1}{M}\\sum_{i=1}^M \\mathbf{1}(\\hat{e}^{(i)} = e^{\\ast (i)})\n",
    "$$\n",
    "\n",
    "其中 $M$ 是测试样本数。\n",
    "\n",
    "**为什么用这个指标（在 EmpDG 的设计下）**\n",
    "\n",
    "* EmpDG 强调**multi-resolution emotion perception**（多分辨率情感感知），模型内部明确有一个情感分类器结构。Emotion Accuracy 测量这一模块是否有效，即模型是否能从对话上下文捕捉到正确的情绪信号，这是共情能力的**前置要素**（理解先于回应）。\n",
    "* 如果模型不能正确识别情感，那么即使 decoder 输出表面上有情感词，也可能不是针对正确情绪（例如把“悲伤”误识为“惊喜”）。\n",
    "\n",
    "**优点**\n",
    "\n",
    "* 直接评估情感理解模块的准确性，是一个可重复、客观的度量。\n",
    "* 在论文里能反映 multi-resolution encoder 的贡献（如果 EmotionAccuracy ↑，说明情感感知模块有效）。\n",
    "\n",
    "**局限**\n",
    "\n",
    "* **并不等于“生成的回复是否表现出共情”**：一个模型可以正确预测情感标签（EmotionAccuracy 高），但生成的回复依然可能不合适或不共情（这就是为什么还需要人工的 Empathy 评分）。\n",
    "* **依赖 gold label 的质量与标签颗粒度**：EmpatheticDialogues 的情感标签是 coarse-grained（有限类别），可能掩盖细微情绪差别。\n",
    "* **评估范围有限**：只衡量“理解”而非“表达”。若要评价回复本身的情感表达，需另行用情感分类器对生成回复进行分类，或用人工评估。\n",
    "\n",
    "**总结**\n",
    "\n",
    "* **PPL** 衡量语言概率拟合（流畅性），但对多解场景不敏感；\n",
    "* **Distinct-1/2** 直观反映表面多样性，能证明对抗训练减轻通用回复，但需结合相关性判断是否“有意义”的多样性；\n",
    "* **Emotion Accuracy（论文中）** 衡量模型从上下文识别情绪的能力（context→emotion），是共情能力的必要前提，但**不等同于**生成回复是否真正表达或实现了共情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\python38.zip', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\DLLs', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg', '', 'C:\\\\Users\\\\惠普\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\win32', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\win32\\\\lib', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\Pythonwin', 'e:\\\\Study\\\\Programs\\\\Anaconda3\\\\envs\\\\empdg\\\\lib\\\\site-packages\\\\setuptools\\\\_vendor', '/apdcephfs/share_916081/qtli/install/ft_local/EmpDG']\n",
      "开始训练 EmoPrepend 模型...\n",
      "配置: emb_dim=300, hidden_dim=300, epochs=100\n",
      "加载数据...\n",
      "LOADING empathetic_dialogue ...\n",
      "[situation]: i spent all weekend working on my truck to fix a miss in the engine . despite spending over $ 200 in parts , it did not do a thing to fix the miss .\n",
      "[emotion]: angry\n",
      "[context]: ['i worked on my truck all weekend . spent $ 215 on parts and a special tool . still did not fix the miss in the engine .']\n",
      "[emotion context]: truck spent parts special fix miss engine\n",
      "[target]: what is the problem with the engine ?\n",
      "[feedback]: it has a miss . sometimes when you are driving it sputters and jerks and barely has any power . other times it drives just fine . i have replaced so many things on it and nothing makes a difference for very long .\n",
      " \n",
      "[situation]: a few years ago , my marriage broke up , and i found myself living alone for the first time in my life . though i eventually grew accustomed to the solitude , it took a while to get used to it .\n",
      "[emotion]: lonely\n",
      "[context]: ['i found myself divorced a few years ago , and for the first time in my life , i was living alone .']\n",
      "[emotion context]: found years ago first time life living\n",
      "[target]: i felt sad and depressed due to my insecurities and felt rejected from society\n",
      "[feedback]: yes , i felt that way for a while . but then i grew used to the solitude .\n",
      " \n",
      "[situation]: i felt proud when i had accomplished a dream that i wanted to do for a long time .\n",
      "[emotion]: proud\n",
      "[context]: ['i felt proud when i had accomplished a dream that i wanted to do for a long time . it was absailing ! !']\n",
      "[emotion context]: felt proud accomplished dream long time\n",
      "[target]: i am not familiar with abseiling , but i am glad that you felt good about it .\n",
      "[feedback]: absailing , its when you rappeling down a building . what have you down that amazing ?\n",
      " \n",
      "train length:  20724\n",
      "valid length:  2972\n",
      "test length:  2713\n",
      "创建模型...\n",
      "Embeddings: 22359 x 300\n",
      "Loading embedding file: vectors/glove.6B.300d.txt\n",
      "Pre-trained: 18064 (80.79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Study\\ml_projects\\EmpDG-master\\run_train.py:305: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('result/' + config.model + '_best.tar', map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL USED EmoPrepend\n",
      "TRAINABLE PARAMETERS 14496563\n",
      "测试模式...\n",
      "testing generation (quiet mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:9.6987; ppl:16297.2: 100%|██████████| 2713/2713 [24:40<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 9.6987, PPL: 24725.8045, Acc: 0.0291; Dist1: 0.0068; Dist2: 0.0213\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from run_train import train_notebook\n",
    "\n",
    "# 示例：仅运行测试（从 result/ 加载模型权重并在测试集上评估）\n",
    "# 注意：请确保存在 result/<MODEL>_best.tar（如 result/EmoPrepend_best.tar）\n",
    "result = train_notebook(\n",
    "    emb_dim=300,\n",
    "    hidden_dim=300,\n",
    "    hop=1,\n",
    "    heads=2,\n",
    "    pretrain_emb=True,\n",
    "    label_smoothing=True,\n",
    "    noam=True,\n",
    "    pointer_gen=True,\n",
    "    test=True,            # 进入测试模式：从 result/ 读取模型权重\n",
    "    enable_test=True,     # 测试完成后返回指标\n",
    "    verbose_test=False,   # 若为 True 会打印每个样例的详细生成\n",
    "    model='EmoPrepend',\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empdg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
